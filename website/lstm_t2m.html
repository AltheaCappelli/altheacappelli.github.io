<!DOCTYPE html>
<html>
<head>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" rel="stylesheet" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    body {
      font-family: Arial, sans-serif;
      background-image: url('images/hintt.png');
      background-repeat: no-repeat;
      background-size: cover;
      background-attachment: fixed;
      padding-bottom: 20px;
    }  

    .code-container {
        position: relative;}
    .copy-button {
        position: absolute;
        top: 5px;
        right: 5px;
        padding: 6px 12px;
        border: none;
        background-color: #838383;
        color: white;
        border-radius: 5px;
        cursor: pointer;
        font-size: 14px;
        opacity: 0.8;
        transition: 0.3s;
      }

      .copy-button:hover {
        opacity: 1;
      }

      /*------tab-------*/
.topnav {
  display: flex;
  padding-top: 3px;
  position: fixed;
}

.nav-button {
  overflow: hidden;
  background-color: transparent;
}

.nav-button a {
  display: inline-block;
  color: black;
  text-align: center;
  padding: 14px 25px;
  text-decoration: none;
  font-size: 18px;
  transition:
    background-image 0.2s ease,
    padding 0.2s ease,
    font-size 0.2s ease;
}

.nav-button a:hover{
  padding: 18px 30px;
  font-size: 22px;
  background-image: conic-gradient(
    #eca4a8 0deg 26deg,
    #d9bba1 26deg 51deg,
    #bfd1a0 51deg 77deg,
    #a3e3a7 77deg 102deg,
    #87eeb3 102deg 128deg,
    #6ff2c3 128deg 154deg,
    #5fedd4 154deg 180deg,
    #57e1e4 180deg 205deg,
    #59cff0 205deg 231deg,
    #65b9f7 231deg 257deg,
    #79a2f7 257deg 282deg,
    #928cef 282deg 308deg,
    #b07ce4 308deg 334deg,
    #e892cc 334deg 360deg
    );
    opacity: 0.7;
    color: white;
}

.nav-button a.active {
  background-image: conic-gradient(
    #eca4a8 0deg 26deg,
    #d9bba1 26deg 51deg,
    #bfd1a0 51deg 77deg,
    #a3e3a7 77deg 102deg,
    #87eeb3 102deg 128deg,
    #6ff2c3 128deg 154deg,
    #5fedd4 154deg 180deg,
    #57e1e4 180deg 205deg,
    #59cff0 205deg 231deg,
    #65b9f7 231deg 257deg,
    #79a2f7 257deg 282deg,
    #928cef 282deg 308deg,
    #b07ce4 308deg 334deg,
    #e892cc 334deg 360deg
    );
    color: white;
}

    /*---------sidebar---------*/
      .sidebar {
        margin: 0;
        margin-top: 70px;
        padding: 0;
        width: 212px;
        background-image: 
          radial-gradient(circle at 50% 50%, #34c1e574 50%, transparent 50.05%),
          radial-gradient(circle at 41% 77%, #a9373735 22%, transparent 22.05%),
          radial-gradient(circle at 13% 25%, #eeff0062 26%, transparent 26.05%),
          linear-gradient(211deg, #ffae00ff 40%, transparent 65.05%);
        position: fixed;
        height: 100%;
        overflow: auto;
      }

      .sidebar a {
        display: block;
        color: black;
        padding: 17px;
        text-decoration: none;
      }
      
      .sidebar a.active {
        background-color: rgba(190, 6, 119, 0.542);
        color: white;
      }

      .sidebar a:hover:not(.active) {
        background-color: rgba(243, 117, 255, 0.391);
        color: white;
      }

      div.content {
        margin-left: 225px;
        padding: 1px 16px;
      }

      @media screen and (max-width: 700px) {
        .sidebar {
          width: 100%;
          height: auto;
          position: relative;
        }
        .sidebar a {float: left;}
        div.content {margin-left: 0;}
      }

      @media screen and (max-width: 400px) {
        .sidebar a {
          text-align: center;
          float: none;
        }
      }

      .button {
        padding: 15px 32px;
        text-align: center;
        display: inline-block;
        font-size: 16px;
        margin: 4px 2px;
        cursor: pointer;
      }

      .Results {
        border-color: rgb(238, 255, 0);
        background-color: #ffffff00;}

      .Code {
        border-color: rgb(238, 255, 0);
      background-color: #ffffff00;}
      
      .Results:hover, .Code:hover,
      .button.active:hover {
        transform: scale(1.02);	
        background-image: url('../images/sonnn.png');
        color: black;
        background-size: cover; 
      }

      .button.active {
      background-image: url('../images/crep.png'); 
      background-size: cover; 
      color: white;}

      .center {
        display: block;
        margin-left: auto;
        margin-right: auto;
        }

    </style>
</head>
<body>

<div class="topnav">
  <div class="nav-button"><a href="https://altheacappelli.github.io">Home</a></div>
    <div class="nav-button">
  <a href="https://altheacappelli.github.io/website/projects.html" class="active">Projects</a></div>
</div>

<div class="sidebar">
  <a href="https://altheacappelli.github.io/website/analysis_t2m.html">Analysis of temperature anomalies in Europe</a>
  <a class="active" href="https://altheacappelli.github.io/website/lstm_t2m.html">LSTM predictions of mean temperature anomalies in Europe</a>
  <a href="https://altheacappelli.github.io/website/conv_t2m.html">Convolutional LSTM predictions of temperature anomalies in Europe</a>
</div>

<div class="content">
<div id="Projects" style="display:block;">
<h1 style="color:rgb(182, 3, 119);"><b>LSTM-based prediction of the mean 2m air temperature anomaly from the <i>Copernicus Climate Data Store</i> (CDS)</b></h1>
    <h3>The goal of this project is to build a simple LSTM neural network that can still perform reasonably well in predicting the mean temperature anomaly in Europe. The model is trained on data from 1940 to 2004 and 
    evaluated on both the training data and the test data from 2005 to 2024. Different layer setups were tested to improve the model's performance. The results of the model's inference on both the training and test data are shown to give an idea of the LSTM accuracy. Specifically, 
    the training data is used to check if the model can reproduce the known patterns, while the test data is used to evaluate how well the model generalizes to unknown data. Finally, the trained network is used to predict future temperature anomalies between 2024 and 2035.   
    </h3>
    <h4>I used the 2m air temperature data from the 
    <a href="https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels?tab=overview" target="_blank">ERA5 Reanalysis dataset</a> 
    provided by the Copernicus Climate Data Store (CDS). ERA5 is a global climate reanalysis dataset that provides hourly estimates of atmospheric variables, including the 2m air temperature.
    </h4>
    <h4>The following plots summarize the main results of the project. You can view the code in the Code section, and you can download the full project <a href="https://github.com/AltheaCappelli/lstm_climate_change_predicter/tree/main" target="_blank">
    from Github</a> to experiment and make your modifications.</h4>
<div class="tab">
<button class="button Results active" onclick="openCity(event, 'Results')">Results</button>
<button class="button Code" onclick="openCity(event, 'Code')">Code</button>
</div>

<div id="Results" class="tabcontent" style="display: block;">
    <h3><b>Accuracy of the lstm neural network</b></h3>
      <p>First, the training and validation losses were plotted. The model is trained on the mean temperature data from 1940 to 2004, and validated on data from 2005 to 2024.</p>
      <img src="../images/lstm_accuracy.svg" width = '60%' alt="Accuracy of the lstm model" class="center">
      <p>The training loss decreases quickly already after a few epochs, indicating that <b>the model is learning fast</b>. However, even if the validation loss takes longer
        to decrease, it eventually falls below the training loss after several more epochs. This could be for different reasons, one being the nature of the data. 
        Indeed, the validation data, which spans from 2005 on, is <b>slightly smoother</b>, it has fewer fluctuations compared to the training data, which in contrary has 
        steeper peaks. As a result, the model probably finds it easier to learn from the validation data. Additionally, the <b>dropout layer</b> is active during training
        but not during validation, as standard in Keras, which could also explain the difference of accuracy. The y-axis of the plot is logarithmically scaled to better 
        visualize the results.</p>
      <p>Despite these differences, both the training and validation losses ultimately reach a <b>sufficiently small error</b>, showing that the model is <b>performing well</b>
        on both datasets.</p>
      <h3><b>Results of the lstm predictions</b></h3>
      <p>Then, the results of the predictions have been plotted together with the MC dropout, which is the mean of 100 forward passes, to be able to estimate the uncertainty ranges of the model's result.</p>
      <img src="../images/lstm_temp_comparison.svg" width = '60%' alt="Accuracy of the lstm model" class="center">
      <p>The plot shows that the <b>mean predictions from MC dropout</b>, in red and orange, <b>closely match the original model predictions</b>, in green and light blue, 
        and are almost overlapping. This is due to the low dropout rate of 0.2, so that the variation across different runs stays small. When comparing the MC uncertainty 
        for inputs from the training period and the validation period, the difference is clear. For inputs from the earlier years, data the model has already seen during 
        training, the <b>uncertainty remains low</b>, showing that the <b>model is confident with known patterns</b>. But in the test period, especially from year 2018 to 2024,
        the <b>uncertainty grows significantly</b>. This is expected, since when the model makes predictions with unknown data, it becomes less certain, a <b>common behavior 
        in time series models</b>, where small differences can accumulate over time. Despite this, both the model’s predictions and the MC dropout mean <b>follow the overall 
        trend</b> of the observed temperature anomalies and match the exponential trend quite well within the uncertainty range. 
        In summary, the plot shows that even a <b>simple LSTM neural network</b> can do a good job at <b>reproducing the trend of rising temperatures across Europe</b>,
        following the exponential pattern quite clearly.</p>
        <h3><b>Estimation of future temperature evolution</b></h3>
      <p>Finally, the trained model is used to forecast the temperature anomalies for the next 10 years.</p>
      <img src="../images/forecast.svg" width = '80%' alt="Result of the future predictions" class="center">  
      <p>The first plot shows the <b>full range of temperature anomalies from 1940 to 2034</b>, comparing the actual data with the predictions from the LSTM model and MC Dropout model, while
        the second plot shows a <b>closer look at the forecast period</b>, focusing on the years from 2024.  
        Both plots show a <b>steep rise</b> in predicted temperature anomalies, especially after 2030. These predicted values are, of course, not realistic and come from the <b>model's 
        simplistic extrapolation of historical trend</b>, without taking into account real-world climate and feedback mechanisms. However, these results should not be interpreted as "literal" future 
        temperatures, as they just show the model’s learned trend, namely a <b>rapid and accelerating increase in temperature anomalies</b>. This also highlights the main point, that even a basic
        time series model shows how fast temperatures are rising in Europe.
      </p>
</div>

<div id="Code" class="tabcontent" style="display: none;">
<h3><b>Creating the LSTM neural network</b></h3>
        <p>Install necessary packages and initialize data</p>

    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        import numpy as np
        import xarray as xr
        import matplotlib.pyplot as plt
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.layers import LSTM, Dense, Dropout

        file = 'data_t2m_tot.nc' #2m air temperature
        ds = xr.open_dataset(file, chunks = {'valid_time' : 50}) #chunks for faster computation
        temp = ds.t2m
      </code></pre>
    </div>
      
    <p>Define key parameters.</p>
    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        yrs = range(1940, 2035)
        cutoff = 66 #year 2005
        forecast = 10 #years to forecast
        epochs = 100
        patience = 25 #early stopping if no improvement after 25 epochs
        baseline = 0.02, #early stopping if improvement stays below this threshold
        start_epoch = 50 #wait 50 epochs before checking for early stopping
        input_size = 5
        batch_size = 4
      </code></pre>
    </div>
    <p>Compute the temperature anomalies as the <b> between the mean temperature for each year and month, relative to the value for the year 1940</b>.</p>
    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        temp_mean = temp.groupby(['valid_time.year']).mean(dim=['valid_time', 'latitude', 'longitude']) # (year)
        anom_year = temp_mean - temp_mean.sel(year = 1940) #lazy compute yearly anomaly
        anom_year_vals = anom_year.values[:-1] #extract values
      </code></pre>
    </div>
    <p>Build the neural network. The model uses an <b>LSTM lneural network</b>, which is optimal for <b>time series data</b> like the one used in this project. 
      It consists of three LSTM layers with <b>up to 30 neurons</b>, which showed good performance during testing. There is a <b>dropout layer</b> before the final layer 
      to reduce the risk of overfitting. Although the dataset is relatively small, adding a low rate dropout layer gives an extra layer of caution and it also allows
      to estimate the uncertainty using <b>Monte Carlo dropout</b>, as mentioned below.
    </p>
    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        def build_lstm(input_size):
            model = Sequential()
            model.add(Input((input_size, 1)))
            model.add(LSTM(5, activation='relu', return_sequences=True))
            model.add(LSTM(15, activation='relu', return_sequences=True))
            model.add(Dropout(0.2)) #low dropout rate to prevent overfitting due to limited training data
            model.add(LSTM(30, activation='relu'))
            model.add(Dense(1))
            model.compile(optimizer='adam', loss='mse')
            model.summary() #summary of the model
            return model

        model_lstm = build_lstm(input_size)
      </code></pre>
    </div>
    <p>An <b>early stopping</b> callback is defined to stop training once the model stops improving, as this helps save computation time and prevents overfitting by avoiding unnecessary epochs.
    </p>
    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        early_stopping = EarlyStopping(monitor = 'val_loss', #monitor the test error
                               patience = patience,
                               verbose = 1, #shows when callback is used
                               baseline = baseline,
                               restore_best_weights = True, #restore weights from the best performing epoch
                               start_from_epoch = start_epoch)
      </code></pre>
    </div>
<p>Define a function to prepare the input data by splitting it into <b>overlapping sequences of fixed length</b>. For example, if given the list [1, 2, 3, 4, 5, 6] 
  and an input size of 3, the function returns the input sequences [[1, 2, 3], [2, 3, 4], [3, 4, 5]], together with their corresponding output values [4, 5, 6].</p>
    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        def prep_data(data, input_size):
            x = np.zeros((len(data) - input_size, input_size))
            for i in range(len(x)):
                x[i] = data[i:i+input_size] #input data
            y = np.array(data[input_size:]) #output data
            return x, y
      </code></pre>
    </div>
  <p>Format the data and prepare it for training and testing.</p>
    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        #split data
        training = anom_year_vals[:cutoff] 
        test = anom_year_vals[cutoff - input_size -1:]  #-1 to let it begin with the last year of training

        #format into input-output pairs
        x_train, y_train = prep_data(training, input_size)
        x_test, y_test = prep_data(test, input_size)
      </code></pre>
    </div>
<p>Train the model.</p>
    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        fit_lstm = model_lstm.fit(x_train, y_train,
                        epochs=epochs,
                        validation_data=(x_test, y_test), #show error
                        callbacks=[early_stopping],
                        shuffle = False,
                        batch_size = batch_size)

        res_test = model_lstm.predict(x_test, verbose=0).flatten()
        res_train = model_lstm.predict(x_train, verbose=0).flatten()
        np.savez("res_LSTM.npz", res_train = res_train, res_test = res_test)

      </code></pre>
    </div>
    <h3><b>Plotting the accuracy and results with MC  dropout</b></h3>
<p>Plot the training and validation (test) loss.</p>
    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        plt.figure(figsize=(8, 5))

        plt.plot(fit_lstm.history['loss'], color='blue', linewidth=2, label="Training Loss")
        plt.plot(fit_lstm.history['val_loss'], color='red', linewidth=2, linestyle='--', label="Validation Loss")

        #axes & title
        plt.yscale('log')
        plt.xticks(fontsize=12)
        plt.yticks(fontsize=12)
        plt.xlabel('Epochs', fontsize=12,  labelpad=10)
        plt.ylabel('Loss', fontsize=12, labelpad=10)
        plt.title('Training and Validation Loss', fontsize=18, fontweight='bold', pad=20)
        plt.grid(True, linestyle='--', alpha=0.6)

        plt.legend(loc='best', fontsize=12, frameon=True, fancybox=True, shadow=True, borderpad=1)
        plt.tight_layout()
        plt.savefig('lstm_accuracy.svg')
        plt.show()
      </code></pre>
    </div>
  <p>Define the <b>Monte Carlo dropout</b> function following <i><a href="https://arxiv.org/abs/1506.02142" target="_blank">Gal & Ghahramani (2015)</a></i> to estimate the uncertainty of the model. 
    The MC dropout keeps the <b>dropout layer active during inference</b>, instead of only during training. Then, by performing multiple forward passes with the same 
    input, the MC dropout generates a <b>distribution of outputs</b>. The mean corresponds to the final prediction, while the variance reflects the model’s confidence.
    Also define an <b>exponential fit</b> to model the warming trend in the temperature anomaly. This is used to compare between the neural network's predictions and the
    <b>overall trend</b>, checking if the model captures the exponential nature of the warming over time.
    </p>
    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        def MC_dropout(model, X, n):
            preds = np.array([model(X, training=True) for i in range(n)])
            mean_preds = preds.mean(axis=0)
            var = preds.var(axis=0)
            return mean_preds.flatten(), var.flatten()

        def exp_fit(x, a, b, c):
            return a*np.exp(b * x) + c
      </code></pre>
    </div>
<p>Compute MC Dropout predictions and fit curve.</p>
    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        n_MC = 100 #forward passes
        mean_preds_train, var_train = MC_dropout(model_lstm, x_train, n_MC)
        mean_preds_test, var_test = MC_dropout(model_lstm, x_test, n_MC)
        std_dev_test = np.sqrt(var_test)  
        std_dev_train = np.sqrt(var_train)
        np.savez("MC_LSTM.npz", std_dev_train = std_dev_train, std_dev_test = std_dev_test, 
        mean_preds_train = mean_preds_train, mean_preds_test = mean_preds_test)
        
        x_fit = np.array(yrs[:-forecast])-1940
        params, _ = curve_fit(exp_fit, x_fit, anom_year_vals, p0= [1, 0.001, 0])
        fit_data = exp_fit(x_fit, *params)      
      </code></pre>
    </div>
    <p>Define key function for plotting.</p>
    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        def Uncertainty_plt(ax, x, y, mean, std, label_y, label_m, color_y, color_m, linewidth):
        ax.plot(x, y,  label=label_y, color=color_y, linewidth= linewidth) #lstm prediction
        ax.plot(x, mean, label=label_m, color=color_m, linewidth= linewidth) #mc dropout prediction
        ax.fill_between(x, mean + std, mean - std, color=color_m, alpha=0.2, linewidth= linewidth) #range of uncertainty      </code></pre>
    </div>
<p>Display the results in a <b>line plot</b>.
  This plot shows the <b>neural network's predictions</b> together with the <b>actual temperature anomalies</b>. The black line represents the observed temperature
  anomalies, while the blue line shows the exponential fit. The orange line represents the inference results of the model for the training and test data.
  The purple line show the results of the MC dropout, here taken as the mean of 100 forward passes. The uncertainty from MC dropout predictions
  is represented by the shaded regions in purple. The background colors indicate the training and test regions.</b>.</p>
    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        plt.figure(figsize = (10,6))

        yrs_train = yrs[input_size:cutoff]
        yrs_test = yrs[cutoff -1 :-forecast]
        
        plt.plot(yrs[:-forecast], anom_year_vals, linestyle='--', color='black', label='CDS Temperature anomaly', linewidth = 1.5)
        plt.plot(yrs[:-forecast], fit_data, color='dodgerblue', linewidth=1.5, label='Exponential fit')
        Uncertainty_plt(plt, yrs_train, res_train, mean_preds_train, std_dev_train, 'LSTM predictions', 'MC Dropout predictions',
                    'darkorange', 'darkviolet', 2)
        Uncertainty_plt(plt, yrs_test, res_test, mean_preds_test, std_dev_test, '', '',
                    'darkorange', 'darkviolet', 2)
        
        #add aesthetics
        plt.vlines(2005, -2, 5, color = 'black', linestyle = '--', alpha = 0.3)
        plt.axvspan(yrs[0], yrs[cutoff - 1], color='lime', alpha=0.1)
        plt.axvspan(yrs[cutoff - 1], yrs[-forecast -1], color='pink', alpha=0.2)
        plt.text(x=1980, y=3.1, s='Training', fontsize = 15, fontweight = 'bold', 
                color='darkgreen', bbox=dict(facecolor='white', edgecolor='darkgreen', boxstyle='round'))
        plt.text(x=2012, y=3.1, s='Test', fontsize = 15, fontweight = 'bold', 
                color='mediumvioletred', bbox=dict(facecolor='white', edgecolor='mediumvioletred', boxstyle='round'))
        plt.grid(True, linestyle='--', alpha=0.6)
        
        #axes & title
        year_ticks = np.arange(1940, 2026, 5)
        plt.xticks(year_ticks, year_ticks, fontsize=10, rotation=45)
        plt.yticks(fontsize=10)
        plt.xlabel('Year', fontsize=12)
        plt.ylabel('Temperature anomaly (°C)', fontsize=12)
        plt.ylim(-0.25, 3.5)
        plt.xlim(yrs[0], yrs[-forecast -1])
        plt.title('Temperature anomalies in Europe and LSTM predictions (1940–2024)', fontsize=14, fontweight='bold', pad=20)
        
        plt.legend(loc='upper left', fontsize=10, frameon=True, fancybox=True, shadow=True, borderpad=1)
        plt.tight_layout()
        plt.savefig('lstm_temp_comparison.svg')
        plt.show()
      </code></pre>
    </div>
    <h3><b>Making future predictions</b></h3>
    <p>With the trained model, it is now possible to make <b>predictions for future years</b> to estimate how the mean temperature anomaly in Europe might change.</p>
    <p>Reshape the input data, so that the model <b>predicts one year at a time</b>. Each new prediction is <b>added to the input sequence</b> to predict the next year.</p>
    <div class="code-container">
      <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
      <pre><code class="language-python">
        last = anom_year_vals[-5:].reshape(1,5) #2024, adjust to match the wanted shape
        preds_future = []
        preds_yrs = 11
        
        for i in range(preds_yrs):
            pred = model_lstm.predict(last, verbose=0).flatten()
            preds_future.append(pred)
            last = np.append(last, pred)[-5:].reshape(1,5)  #new input
        
        preds_future = np.reshape(preds_future, preds_yrs)
      </code></pre>
    </div>
<p>Compute the mean MC Dropout forecast similarly, but this time using the <b>MC Dropout prediction</b> function instead of the standard LSTM model.</p>
<div class="code-container">
  <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
  <pre><code class="language-python">
    last = anom_year_vals[-5:].reshape(1,5) #2024, adjust to match the wanted shape
    preds_future = []
    preds_yrs = 11
    
    for i in range(preds_yrs):
        pred = model_lstm.predict(last, verbose=0).flatten()
        preds_future.append(pred)
        last = np.append(last, pred)[-5:].reshape(1,5)  #new input
    
    preds_future = np.reshape(preds_future, preds_yrs)
    np.savez("future_LSTM.npz", preds_future = preds_future,
        preds_future_MC = preds_future_MC, std_dev_MC = std_dev_MC)
  </code></pre>
</div>
<p>Plot the results in a <b>line plot</b>.
  The first plot shows the <b>full range of temperature anomalies from 1940 to 2034</b>, comparing the actual data with the predictions from the LSTM model and MC Dropout model. 
  The LSTM predictions are shown in orange, while the MC Dropout results are shown in purple, together with shaded uncertainty regions representing the prediction accuracy. 
  The second plot shows a <b>closer look at the forecast period</b>, focusing on the years from 2024. It highlights the predicted anomalies from both models and their uncertainty regions, 
  for a more detailed examination of the extrapolated trends. Both plots include the training, test, and forecast phases.</p>
  <div class="code-container">
    <button class="copy-button" onclick="copyToClipboard(this)">Copy</button>
    <pre><code class="language-python">
      fig, (ax_tot, ax_fut) = plt.subplots(1, 2, figsize=(14, 6), sharey= True)

      yrs_forec = yrs[-forecast -1:]
      
      #all the years
      ax_tot.plot(yrs[:-forecast], anom_year_vals, linestyle='--', color='black', label='CDS Temperature anomaly', linewidth = 1.5)
      Uncertainty_plt(ax_tot, yrs_train, res_train, mean_preds_train, std_dev_train, 'LSTM predictions', 'MC Dropout predictions', 'darkorange', 'darkviolet', 2)
      Uncertainty_plt(ax_tot, yrs_test, res_test, mean_preds_test, std_dev_test, '', '', 'darkorange', 'darkviolet', 2)
      Uncertainty_plt(ax_tot, yrs_forec, preds_future, preds_future_MC, std_dev_MC, '', '', 'darkorange', 'darkviolet', 2)
      
      #zoom on forecast
      Uncertainty_plt(ax_fut, yrs_forec, preds_future, preds_future_MC, std_dev_MC, '', '', 'darkorange', 'darkviolet', 2)
      
      #add aesthetics
      ax_tot.vlines(2005, -5, 65, color = 'black', linestyle = '--', alpha = 0.3)
      ax_tot.vlines(2024, -5, 65, color = 'black', linestyle = '--', alpha = 0.3)
      ax_tot.axvspan(yrs_train[0]-input_size, yrs_train[-1], color='lime', alpha=0.1)
      ax_tot.axvspan(yrs_test[0], yrs_test[-1], color='pink', alpha=0.2)
      ax_tot.axvspan(yrs_forec[0], yrs_forec[-1], color='gold', alpha=0.2)
      ax_fut.axvspan(yrs_forec[0], yrs_forec[-1], color='gold', alpha=0.2)
      ax_tot.text(x=1980, y=57, s='Training', fontsize = 15, fontweight = 'bold', 
              color='darkgreen', bbox=dict(facecolor='white', edgecolor='darkgreen', boxstyle='round'))
      ax_tot.text(x=2011, y=57, s='Test', fontsize = 15, fontweight = 'bold', 
              color='mediumvioletred', bbox=dict(facecolor='white', edgecolor='mediumvioletred', boxstyle='round'))
      ax_tot.text(x=2015, y=45, s='Forecast', fontsize = 15, fontweight = 'bold', 
              color='goldenrod', bbox=dict(facecolor='white', edgecolor='goldenrod', boxstyle='round'))
      ax_fut.text(2032.5, preds_future_MC[-1]-1, f"{preds_future_MC[-1]:.2f}°C", fontsize=12, color = 'indigo', fontweight = 'bold')
      ax_fut.plot(yrs[-1], preds_future_MC[-1], color = 'darkviolet', marker = 'o')
      ax_fut.text(2032.5, preds_future[-1]-1, f"{preds_future[-1]:.2f}°C", fontsize=12, color = 'orangered', fontweight = 'bold')
      ax_fut.plot(yrs[-1], preds_future[-1], color = 'darkorange', marker = 'o')
      ax_tot.grid(True, linestyle='--', alpha=0.6)
      ax_fut.grid(True, linestyle='--', alpha=0.6)
      
      #axes & title
      ax_fut.set_yticks(np.arange(0,61,5), np.arange(0,61,5))
      ax_fut.set_xticks(yrs_forec, yrs_forec, rotation = 45)
      ax_tot.set_xticks(yrs[::5], yrs[::5], rotation = 45)
      ax_tot.set_xlim(1940,2035.5)
      ax_fut.set_xlim(2024,2034.2)
      ax_tot.set_ylim(-1,65)
      ax_tot.set_ylabel('Temperature anomaly (°C)')
      ax_tot.legend(fontsize=9, frameon=True, fancybox=True)
      ax_tot.set_xlabel('Year')
      ax_fut.set_title('Zoom 2024 - 2034', fontweight = 'bold')
      ax_tot.set_title('1940 – 2034', fontweight = 'bold')
      fig.suptitle('Temperature anomalies in Europe (1940-2024) and LSTM forecast (2024-2034)', fontsize=15, fontweight='bold', y=1.05)
      
      plt.tight_layout()
      plt.subplots_adjust(top=0.93)
      plt.savefig('forecast.svg')
      plt.show()      
    </code></pre>
  </div>
</div>
</div>

<script>
function openCity(evt, cityName) {
  const tabcontent = document.getElementsByClassName("tabcontent");
  for (let i = 0; i < tabcontent.length; i++) {
    tabcontent[i].style.display = "none";
  }
  const tablinks = document.getElementsByClassName("button");
  for (let i = 0; i < tablinks.length; i++) {
    tablinks[i].classList.remove("active");
  }
  document.getElementById(cityName).style.display = "block";
  evt.currentTarget.classList.add("active");
}

function copyToClipboard(button) {
  const codeBlock = button.nextElementSibling.innerText;
  navigator.clipboard.writeText(codeBlock).then(() => {
    button.innerText = "Copied!";
    setTimeout(() => {
      button.innerText = "Copy";
    }, 2000);
  });
}

</script>
</body>
</html>
